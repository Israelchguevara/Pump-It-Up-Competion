{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AjphXOH7L7zE",
    "outputId": "ffb083c5-38d4-45c8-d26b-18fea262cf9a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbSiRUvj8Ir4"
   },
   "source": [
    "**Importamos Librerias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SXlgYkMBL_HX",
    "outputId": "b02926c1-0de3-4a3f-d762-23c42f3c2f4d"
   },
   "outputs": [],
   "source": [
    "!pip install ydata-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9mjfJTcLmhz"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ydata_profiling import ProfileReport\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "813V__gRfpLl"
   },
   "source": [
    "**Descripción del Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3somgMQOef0Z"
   },
   "source": [
    "* amount_tsh - Total static head (amount water available to waterpoint)\n",
    "* date_recorded - The date the row was entered\n",
    "* funder - Who funded the well\n",
    "* gps_height - Altitude of the well\n",
    "* installer - Organization that installed the well\n",
    "* longitude - GPS coordinate\n",
    "* latitude - GPS coordinate\n",
    "* wpt_name - Name of the waterpoint if there is one\n",
    "* num_private -\n",
    "* basin - Geographic water basin\n",
    "* subvillage - Geographic location\n",
    "* region - Geographic location\n",
    "* region_code - Geographic location (coded)\n",
    "* district_code - Geographic location (coded)\n",
    "* lga - Geographic location\n",
    "* ward - Geographic location\n",
    "* population - Population around the well\n",
    "* public_meeting - True/False\n",
    "* recorded_by - Group entering this row of data\n",
    "* scheme_management - Who operates the waterpoint\n",
    "* scheme_name - Who operates the waterpoint\n",
    "* permit - If the waterpoint is permitted\n",
    "* construction_year - Year the waterpoint was constructed\n",
    "* extraction_type - The kind of extraction the waterpoint uses\n",
    "* extraction_type_group - The kind of extraction the waterpoint uses\n",
    "* extraction_type_class - The kind of extraction the waterpoint uses\n",
    "*management - How the waterpoint is managed\n",
    "*management_group - How the waterpoint is managed\n",
    "* payment - What the water costs\n",
    "* payment_type - What the water costs\n",
    "* water_quality - The quality of the water\n",
    "* quality_group - The quality of the water\n",
    "* quantity - The quantity of water\n",
    "* quantity_group - The quantity of water\n",
    "* source - The source of the water\n",
    "* source_type - The source of the water\n",
    "* source_class - The source of the water\n",
    "* waterpoint_type - The kind of waterpoint\n",
    "* waterpoint_type_group - The kind of waterpoint\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GbArVbg8P5K"
   },
   "source": [
    "**Cargamos Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJWFkxe3Si2q"
   },
   "outputs": [],
   "source": [
    "train_values = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train_pump_it.csv')\n",
    "train_labels = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/labels_pump_it.csv')\n",
    "test_values = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a048ZoK360n3",
    "outputId": "7c990380-ef4d-497f-aab1-d14666e584db"
   },
   "outputs": [],
   "source": [
    "# Dimensiones (número de filas y columnas) de tres DataFrames\n",
    "\n",
    "print(\"Forma del conjunto de entrenamiento:\", train_values.shape)\n",
    "print(\"Forma del conjunto de etiquetas:\", train_labels.shape)\n",
    "print(\"Forma del conjunto de prueba:\", test_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA5u49TzABQX",
    "outputId": "9872bd1b-ed82-4b02-8757-2b432527c16e"
   },
   "outputs": [],
   "source": [
    "\n",
    "for i  in train_values.columns:\n",
    "    print(train_values[i].value_counts())\n",
    "\n",
    "for i in train_labels.columns:\n",
    "    print(train_labels[i].value_counts())\n",
    "\n",
    "for i in test_values.columns:\n",
    "    print(test_values[i].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85BCRJrS-e5A"
   },
   "source": [
    "**Verificación de Valores Duplicados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WFwpKhx7mRe",
    "outputId": "401d52ae-acb0-49f8-987b-f81d27166379"
   },
   "outputs": [],
   "source": [
    "print(train_values['id'].duplicated().value_counts())\n",
    "print(train_labels['id'].duplicated().value_counts())\n",
    "print(test_values['id'].duplicated().value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRJgNQ4EKAp7"
   },
   "source": [
    "**ID´s contenidos en las tablas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8o50EDHC77r2",
    "outputId": "cbd1b1ac-5b18-4890-fca9-03db0bf6cbe8"
   },
   "outputs": [],
   "source": [
    "pd.merge(train_values, train_labels,indicator=True )['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWgziEr4K5VR",
    "outputId": "1b29ba75-fa5e-4332-ae69-ef6156dd893b"
   },
   "outputs": [],
   "source": [
    "print(train_values['id'].isin(train_labels['id']).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21iVd6DALSOE"
   },
   "source": [
    "**Merge de Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_wVeHpL5oRh"
   },
   "outputs": [],
   "source": [
    "# Unir valores y etiquetas para el conjunto de entrenamiento\n",
    "\n",
    "df_train = pd.merge(train_values,\n",
    "                    train_labels,\n",
    "                    how='left', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZokTWfChLlzQ",
    "outputId": "63a6b700-9ad2-40e6-cff4-5a2b8a36b7bd"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GKo58JUjRiHX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Tzo5mYGRjcD"
   },
   "source": [
    "**EDA  df_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pkhxkGVRlsw"
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(df_train,title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V9l3XvBvSC38",
    "outputId": "69d130d3-2473-471c-84b6-0d28dc3040e6"
   },
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNBwh6pGMYxU",
    "outputId": "ecc163fb-4d60-48d2-f621-8be011564bbe"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "349P54emQ4SE",
    "outputId": "0092f80f-1ac1-4ee6-8f92-48f48444312a"
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLAsiNW0NMmh",
    "outputId": "a4816971-a83a-479a-bd39-aed0ef9def5c"
   },
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NejR_lE5VYiS"
   },
   "source": [
    "**Pre procesado df_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvzq_u9ndFCb",
    "outputId": "9bd5d256-980d-41ea-833d-afc21e326391"
   },
   "outputs": [],
   "source": [
    "# Identificar variables con valores nulos y su porcentaje\n",
    "null_counts = df_train.isnull().sum()\n",
    "null_percentages = (null_counts / len(df_train)) * 100\n",
    "\n",
    "null_info = pd.DataFrame({\n",
    "    'Null Count': null_counts,\n",
    "    'Null Percentage': null_percentages\n",
    "})\n",
    "\n",
    "# Filtro para mostrar solo las columnas con valores nulos\n",
    "null_info_with_nans = null_info[null_info['Null Count'] > 0].sort_values(by='Null Percentage', ascending=False)\n",
    "\n",
    "print(\"Variables con valores nulos y su porcentaje en df_train:\")\n",
    "null_info_with_nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCmdpL4Y4d2D",
    "outputId": "eeef1777-2dc6-419a-d224-0c0184edfbc4"
   },
   "outputs": [],
   "source": [
    "# Identificar columnas con alta cardinalidad\n",
    "high_cardinality_cols = [col for col in df_train.columns if df_train[col].dtype == 'object' and df_train[col].nunique() > 50]\n",
    "\n",
    "print(\"\\nColumnas con alta cardinalidad (redundantes o irrelevantes):\")\n",
    "print(high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ukItB-X5H3a"
   },
   "outputs": [],
   "source": [
    "# Eliminar columnas irrelevantes\n",
    "df_train = df_train.drop(columns=['num_private'], errors='ignore')\n",
    "test_values = test_values.drop(columns=['num_private'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6C5bVNQ_ceO"
   },
   "outputs": [],
   "source": [
    "# Imputación de Valores Faltantes con Unknow\n",
    "for col in ['scheme_name','funder', 'installer']:\n",
    "        df_train[col] = df_train[col].fillna('unknown')\n",
    "        top_n = 50 # Ajusta este número según experimentación\n",
    "        top_frequent_values = df_train[col].value_counts().head(top_n).index.tolist()\n",
    "        df_train[col] = df_train[col].apply(lambda x: x if x in top_frequent_values else 'other_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6e65l7rgUGT",
    "outputId": "985efbe6-ab98-40e4-e526-f21f491bc28a"
   },
   "outputs": [],
   "source": [
    "# Rellenar valores nulos en 'scheme_management' con la moda\n",
    "mode_scheme_management = df_train['scheme_management'].mode()[0]\n",
    "df_train['scheme_management'].fillna(mode_scheme_management, inplace=True)\n",
    "\n",
    "# Rellenar valores nulos en 'public_meeting' con la moda\n",
    "mode_public_meeting = df_train['public_meeting'].mode()[0]\n",
    "df_train['public_meeting'].fillna(mode_public_meeting, inplace=True)\n",
    "\n",
    "# Rellenar valores nulos en 'permit' con la moda\n",
    "mode_permit = df_train['permit'].mode()[0]\n",
    "df_train['permit'].fillna(mode_permit, inplace=True)\n",
    "\n",
    "# Verificar si quedan valores nulos en las columnas tratadas\n",
    "print(\"\\nValores nulos después del tratamiento:\")\n",
    "print(df_train[['scheme_name','scheme_management', 'installer', 'funder', 'public_meeting', 'permit']].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCYa4lGRzSDW"
   },
   "outputs": [],
   "source": [
    "# Manejo de Nulos y 0s en Numéricas\n",
    "if 'gps_height' in df_train.columns:\n",
    "    df_train['is_gps_height_zero'] = (df_train['gps_height'] == 0).astype(int)\n",
    "    # Calculo de la media basado en valores non-zero\n",
    "    median_gps_height = df_train[df_train['gps_height'] != 0]['gps_height'].median()\n",
    "    df_train['gps_height'] = df_train['gps_height'].replace(0, median_gps_height)\n",
    "\n",
    "if 'population' in df_train.columns:\n",
    "    df_train['is_population_zero'] = (df_train['population'] == 0).astype(int)\n",
    "    # Calculo de la media basado en valores non-zero\n",
    "    median_population = df_train[df_train['population'] != 0]['population'].median()\n",
    "    df_train['population'] = df_train['population'].replace(0, median_population)\n",
    "\n",
    "if 'amount_tsh' in df_train.columns:\n",
    "    df_train['is_amount_tsh_zero'] = (df_train['amount_tsh'] == 0).astype(int)\n",
    "    # df_train['amount_tsh'] = df_train['amount_tsh'].apply(lambda x: np.log1p(x)) # Considerar log1p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kn2GqGHazto1"
   },
   "outputs": [],
   "source": [
    "# Ingeniería de Características de Tiempo y Manejo de construction_year\n",
    "if 'date_recorded' in df_train.columns:\n",
    "    df_train['date_recorded'] = pd.to_datetime(df_train['date_recorded'])\n",
    "    df_train['year_recorded'] = df_train['date_recorded'].dt.year\n",
    "    df_train['month_recorded'] = df_train['date_recorded'].dt.month\n",
    "    df_train['day_recorded'] = df_train['date_recorded'].dt.day\n",
    "    df_train['day_of_week'] = df_train['date_recorded'].dt.dayofweek\n",
    "    df_train = df_train.drop(columns=['date_recorded']) # Drop the original date_recorded column\n",
    "\n",
    "if 'construction_year' in df_train.columns:\n",
    "    df_train['is_construction_year_zero'] = (df_train['construction_year'] == 0).astype(int)\n",
    "    # Calculo de la media basado en valores non-zero\n",
    "    median_construction_year = df_train[df_train['construction_year'] != 0]['construction_year'].median()\n",
    "    df_train['construction_year'] = df_train['construction_year'].replace(0, median_construction_year)\n",
    "    if 'year_recorded' in df_train.columns:\n",
    "        df_train['age_of_pump'] = df_train['year_recorded'] - df_train['construction_year']\n",
    "        df_train['age_of_pump'] = df_train['age_of_pump'].apply(lambda x: max(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSItdV0VmSAN"
   },
   "outputs": [],
   "source": [
    "# Eliminación de estas columnas demasiado ruidosas\n",
    "df_train[['lga', 'ward', 'subvillage']] = df_train[['lga', 'ward', 'subvillage']].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1cNxYJu9mqyR",
    "outputId": "dc7018ca-d6aa-4214-82e9-664bc93d1ab2"
   },
   "outputs": [],
   "source": [
    "#  Manejo de Longitud/Latitud con 0s\n",
    "if 'longitude' in df_train.columns:\n",
    "    df_train['is_longitude_zero'] = (df_train['longitude'] == 0).astype(int)\n",
    "    # Imputar 0s en longitud (que son errores para Tanzania) con la mediana\n",
    "    # Se calcula la mediana excluyendo los valores 0\n",
    "    median_longitude = df_train[df_train['longitude'] != 0]['longitude'].median()\n",
    "    # Se reemplazan los valores 0 por la mediana calculada\n",
    "    df_train['longitude'] = df_train['longitude'].replace(0, median_longitude)\n",
    "\n",
    "if 'latitude' in df_train.columns:\n",
    "    # Para la latitud, los valores cercanos a 0 pueden ser válidos,\n",
    "    # pero valores exactamente 0 son errores para la ubicación en Tanzania\n",
    "    # Se identifica si la latitud es 0 y se crea una nueva columna\n",
    "    df_train['is_latitude_zero'] = (df_train['latitude'] == 0).astype(int)\n",
    "    # Imputar 0s en latitud con la mediana de los valores non-zero\n",
    "    # Se calcula la mediana excluyendo los valores 0\n",
    "    median_latitude = df_train[df_train['latitude'] != 0]['latitude'].median()\n",
    "    # Se reemplazan los valores 0 por la mediana calculada\n",
    "    df_train['latitude'] = df_train['latitude'].replace(0, median_latitude)\n",
    "\n",
    "# Verificar si quedan valores 0 en longitud y latitud después del tratamiento\n",
    "print(\"\\nNúmero de 0s en 'longitude' después del tratamiento:\", (df_train['longitude'] == 0).sum())\n",
    "print(\"Número de 0s en 'latitude' después del tratamiento:\", (df_train['latitude'] == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRYoNDgaoecO"
   },
   "outputs": [],
   "source": [
    "# Eliminación de Columnas Redundantes o Irrelevantes ---\n",
    "columns_to_drop = [\n",
    "    'wpt_name', # Identificadores y nombres\n",
    "    'quantity',         # Redundante con 'quantity_group'\n",
    "    'extraction_type', 'extraction_type_class', # 'extraction_type_group' suele ser suficiente\n",
    "    'water_quality',    # Redundante con 'quality_group'\n",
    "    'source_type', 'source_class', # 'source' suele ser suficiente\n",
    "    'waterpoint_type',  # Redundante con 'waterpoint_type_group'\n",
    "    'management_group'  # Redundante con 'management'\n",
    "]\n",
    "df_train = df_train.drop(columns=columns_to_drop, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxAnl_Xh8RTH",
    "outputId": "4a62252b-12c3-479b-91b3-3e25a0324ea8"
   },
   "outputs": [],
   "source": [
    "# Muestra los valores Nan\n",
    "\n",
    "print(\"Valores nulos después del preprocesado:\")\n",
    "print(df_train.isnull().sum()[df_train.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r774momL4FEa",
    "outputId": "4191a681-ee9d-4cf8-dd47-19ee70c0b0b6"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1gpZHJ0pbeZL"
   },
   "source": [
    "**EDA test_values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ej_JeXVr9d1"
   },
   "outputs": [],
   "source": [
    "profile = ProfileReport(test_values,title=\"Profiling Report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BlqcQDbBsBXV",
    "outputId": "330d9fb1-fdea-432e-ea9d-31c793d0c645"
   },
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVlkr-T1bd2f",
    "outputId": "dc7b49d2-85e6-4944-b08c-727122f610de"
   },
   "outputs": [],
   "source": [
    "test_values.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB7ggN2Gbmvo",
    "outputId": "4d76474a-52ee-4f2f-d46c-e75c5f142d4d"
   },
   "outputs": [],
   "source": [
    "test_values.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfBt_rhEaAgK"
   },
   "source": [
    "**Pre procesado test_values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQhjCQ79YmxW",
    "outputId": "dd74ee44-44a3-439b-d48d-8316a19630bd"
   },
   "outputs": [],
   "source": [
    "# Identificar variables con valores nulos y su porcentaje\n",
    "null_counts_test = test_values.isnull().sum()\n",
    "null_percentages_test = (null_counts_test / len(test_values)) * 100\n",
    "\n",
    "null_info_test = pd.DataFrame({\n",
    "    'Null Count': null_counts_test,\n",
    "    'Null Percentage': null_percentages_test\n",
    "})\n",
    "\n",
    "# Filtro para mostrar solo las columnas con valores nulos\n",
    "null_info_with_nans_test = null_info_test[null_info_test['Null Count'] > 0].sort_values(by='Null Percentage', ascending=False)\n",
    "\n",
    "print(\"Variables con valores nulos y su porcentaje en test_values:\")\n",
    "null_info_with_nans_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GvLyddOSYyjq",
    "outputId": "8740c955-39f2-497c-bd1e-8d22c01a10cd"
   },
   "outputs": [],
   "source": [
    "# Identificar columnas con alta cardinalidad\n",
    "high_cardinality_cols_test = [col for col in test_values.columns if test_values[col].dtype == 'object' and test_values[col].nunique() > 50]\n",
    "\n",
    "print(\"\\nColumnas con alta cardinalidad (redundantes o irrelevantes) en test_values:\")\n",
    "print(high_cardinality_cols_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q-hn7RMY3c5"
   },
   "outputs": [],
   "source": [
    "# Imputación de Valores Faltantes con Unknow en test_values\n",
    "for col in ['scheme_name','funder', 'installer']:\n",
    "        test_values[col] = test_values[col].fillna('unknown')\n",
    "\n",
    "        # Reutilizar los top_frequent_values calculados en el df_train\n",
    "        top_frequent_values_train = df_train[col].value_counts().head(50).index.tolist()\n",
    "        if 'other_category' not in top_frequent_values_train:\n",
    "            top_frequent_values_train.append('other_category')\n",
    "\n",
    "        test_values[col] = test_values[col].apply(lambda x: x if x in top_frequent_values_train else 'other_category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rji5kq-rY9qU",
    "outputId": "f0fabf6f-9d94-49fd-98ab-82538a19e150"
   },
   "outputs": [],
   "source": [
    "# Rellenar valores nulos en 'scheme_management' con la moda del train_values\n",
    "mode_scheme_management_train = df_train['scheme_management'].mode()[0]\n",
    "test_values['scheme_management'].fillna(mode_scheme_management_train, inplace=True)\n",
    "\n",
    "# Rellenar valores nulos en 'public_meeting' con la moda del train_values\n",
    "mode_public_meeting_train = df_train['public_meeting'].mode()[0]\n",
    "test_values['public_meeting'].fillna(mode_public_meeting_train, inplace=True)\n",
    "\n",
    "# Rellenar valores nulos en 'permit' con la moda del train_values\n",
    "mode_permit_train = df_train['permit'].mode()[0]\n",
    "test_values['permit'].fillna(mode_permit_train, inplace=True)\n",
    "\n",
    "# Verificar si quedan valores nulos en las columnas tratadas\n",
    "print(\"\\nValores nulos después del tratamiento en test_values:\")\n",
    "print(test_values[['scheme_name', 'scheme_management', 'installer', 'funder', 'public_meeting', 'permit']].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JpiRriDWZCY4"
   },
   "outputs": [],
   "source": [
    "# Manejo de Nulos y 0s en Numéricas en test_values\n",
    "if 'gps_height' in test_values.columns:\n",
    "    test_values['is_gps_height_zero'] = (test_values['gps_height'] == 0).astype(int)\n",
    "    # Usar la mediana calculada del df_train\n",
    "    test_values['gps_height'] = test_values['gps_height'].replace(0, median_gps_height)\n",
    "\n",
    "if 'population' in test_values.columns:\n",
    "    test_values['is_population_zero'] = (test_values['population'] == 0).astype(int)\n",
    "    # Usar la mediana calculada del df_train\n",
    "    test_values['population'] = test_values['population'].replace(0, median_population)\n",
    "\n",
    "if 'amount_tsh' in test_values.columns:\n",
    "    test_values['is_amount_tsh_zero'] = (test_values['amount_tsh'] == 0).astype(int)\n",
    "    # test_values['amount_tsh'] = test_values['amount_tsh'].apply(lambda x: np.log1p(x)) # Considerar log1p si se aplicó en train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XwGec89HZHrt"
   },
   "outputs": [],
   "source": [
    "# Ingeniería de Características de Tiempo y Manejo de construction_year en test_values\n",
    "if 'date_recorded' in test_values.columns:\n",
    "    test_values['date_recorded'] = pd.to_datetime(test_values['date_recorded'])\n",
    "    test_values['year_recorded'] = test_values['date_recorded'].dt.year\n",
    "    test_values['month_recorded'] = test_values['date_recorded'].dt.month\n",
    "    test_values['day_recorded'] = test_values['date_recorded'].dt.day\n",
    "    test_values['day_of_week'] = test_values['date_recorded'].dt.dayofweek\n",
    "    test_values = test_values.drop(columns=['date_recorded']) # Drop the original date_recorded column\n",
    "\n",
    "if 'construction_year' in test_values.columns:\n",
    "    test_values['is_construction_year_zero'] = (test_values['construction_year'] == 0).astype(int)\n",
    "    # Usar la mediana calculada del df_train\n",
    "    test_values['construction_year'] = test_values['construction_year'].replace(0, median_construction_year)\n",
    "    if 'year_recorded' in test_values.columns:\n",
    "        test_values['age_of_pump'] = test_values['year_recorded'] - test_values['construction_year']\n",
    "        test_values['age_of_pump'] = test_values['age_of_pump'].apply(lambda x: max(0, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbcS7t0wUTP2",
    "outputId": "8b367768-9575-475b-cbce-8ecf2c1c10cb"
   },
   "outputs": [],
   "source": [
    "# Eliminación de estas columnas demasiado ruidosas en test_values\n",
    "test_values[['lga', 'ward', 'subvillage']] = test_values[['lga', 'ward', 'subvillage']].fillna('unknown')\n",
    "\n",
    "#  Manejo de Longitud/Latitud con 0s en test_values\n",
    "if 'longitude' in test_values.columns:\n",
    "    test_values['is_longitude_zero'] = (test_values['longitude'] == 0).astype(int)\n",
    "    # Usar la mediana calculada del df_train\n",
    "    test_values['longitude'] = test_values['longitude'].replace(0, median_longitude)\n",
    "\n",
    "if 'latitude' in test_values.columns:\n",
    "    test_values['is_latitude_zero'] = (test_values['latitude'] == 0).astype(int)\n",
    "    # Usar la mediana calculada del df_train\n",
    "    test_values['latitude'] = test_values['latitude'].replace(0, median_latitude)\n",
    "\n",
    "# Verificar si quedan valores 0 en longitud y latitud después del tratamiento\n",
    "print(\"\\nNúmero de 0s en 'longitude' después del tratamiento en test_values:\", (test_values['longitude'] == 0).sum())\n",
    "print(\"Número de 0s en 'latitude' después del tratamiento en test_values:\", (test_values['latitude'] == 0).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU0vrjsSZPyg",
    "outputId": "34a9089d-0d3b-4fe1-fc0a-3cd38d3a429d"
   },
   "outputs": [],
   "source": [
    "# Eliminación de Columnas Redundantes o Irrelevantes en test_values ---\n",
    "test_values = test_values.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "print(\"\\nValores nulos después del preprocesado en test_values:\")\n",
    "print(test_values.isnull().sum()[test_values.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nx2WIR3UZbgW",
    "outputId": "169d153d-39a0-4b4e-dba5-ef0f98371897"
   },
   "outputs": [],
   "source": [
    "test_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pekryVXNebXQ",
    "outputId": "0e540fcc-8505-4ff3-c720-e3bd016f2e21"
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2S2RRaYh-hBh",
    "outputId": "925dc5f9-862b-478e-e5e5-efc776a44aea"
   },
   "outputs": [],
   "source": [
    "# Preparación del Pipeline de Preprocesamiento y División de Datos\n",
    "\n",
    "!pip install lightgbm\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split\n",
    "\n",
    "# Eliminar la columna 'id' ya que no es una característica para el modelo\n",
    "X = df_train.drop(columns=['status_group', 'id'])\n",
    "y = df_train['status_group']\n",
    "\n",
    "# Identificar columnas categóricas y numéricas (re-evaluar si se añadieron o quitaron)\n",
    "categorical_features = X.select_dtypes(include=['object', 'bool']).columns\n",
    "numeric_features = X.select_dtypes(include=np.number).columns\n",
    "\n",
    "# Crear el pipeline de preprocesamiento para características numéricas\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Para LightGBM, se pueden usar One-Hot Encoding o que maneje directamente categóricas.\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' crucial\n",
    "])\n",
    "\n",
    "# Combinar los transformadores usando ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough' # Mantener otras columnas si las hay (aunque con drop inicial no debería)\n",
    ")\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Definir el modelo LightGBM\n",
    "# Usamos objective='multiclass' ya que tenemos 3 clases.\n",
    "# num_class=3 porque hay 3 estados posibles.\n",
    "# boosting_type='gbdt' (Gradient Boosting Decision Tree) es el predeterminado y común.\n",
    "model_lgbm = lgb.LGBMClassifier(objective='multiclass', num_class=3, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Crear el pipeline completo incluyendo preprocesamiento y modelo LightGBM\n",
    "full_pipeline_lgbm = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                       ('classifier', model_lgbm)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6D_Jspow_UC3",
    "outputId": "2deb4ab2-c9b2-4c42-a2c6-50e40cf2a306"
   },
   "outputs": [],
   "source": [
    "# Entrenar el modelo LightGBM\n",
    "# LightGBM puede manejar mejor los datos transformados si le pasamos los nombres de las columnas\n",
    "# categóricas originales antes del OHE, pero con el pipeline de preprocesamiento,\n",
    "# el OHE se aplica internamente. Entrenamos el pipeline completo.\n",
    "full_pipeline_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Crear el archivo de submission para LightGBM\n",
    "# Hacer predicciones en el conjunto de prueba\n",
    "test_ids = test_values['id']\n",
    "X_test = test_values.drop(columns=['id'])\n",
    "\n",
    "# Alinear las columnas de X_test con las de X_train antes de la transformación\n",
    "X_test_aligned, X_train_aligned = X_test.align(X_train, join='inner', axis=1)\n",
    "\n",
    "test_predictions_lgbm = full_pipeline_lgbm.predict(X_test_aligned)\n",
    "submission_df_lgbm = pd.DataFrame({'id': test_ids, 'status_group': test_predictions_lgbm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZYThHTdhpV3O",
    "outputId": "4c80f8d9-3364-410a-fe32-e5f9f7487e5e"
   },
   "outputs": [],
   "source": [
    "# Guardar el archivo de submission\n",
    "submission_df_lgbm.to_csv('submission_lgbm.csv', index=False)\n",
    "\n",
    "print(submission_df_lgbm.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9qSus0ZqJUR",
    "outputId": "cc1795e9-2af0-49b1-9123-fe1f3f14ebae"
   },
   "outputs": [],
   "source": [
    "# prompt: descargar el archivo en csv\n",
    "\n",
    "from google.colab import files\n",
    "files.download('submission_lgbm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdDy3h8mJYI7"
   },
   "source": [
    "![removed-inline-image](assets/removed_inline_image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2egUmKUBqku"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
